{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file illustrates how you might experiment with the HMM interface.\n",
    "You can paste these commands in at the Python prompt, or execute `test_en.py` directly.\n",
    "A notebook interface is nicer than the plain Python prompt, so we provide\n",
    "a notebook version of this file as `test_en.ipynb`, which you can open with\n",
    "`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import TaggedCorpus\n",
    "from eval import eval_tagging, model_cross_entropy, viterbi_error_rate\n",
    "from hmm import HiddenMarkovModel\n",
    "from crf import ConditionalRandomField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.root.setLevel(level=logging.INFO)\n",
    "log = logging.getLogger(\"test_en\")       # For usage, see findsim.py in earlier assignment.\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch working directory to the directory where the data live.  You may need to edit this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(entrain)=8064  len(ensup)=4051  len(endev)=996\n"
     ]
    }
   ],
   "source": [
    "entrain = TaggedCorpus(Path(\"ensup\"), Path(\"enraw\"))                               # all training\n",
    "ensup =   TaggedCorpus(Path(\"ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n",
    "print(f\"{len(entrain)=}  {len(ensup)=}  {len(endev)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 95936 tokens from ensup\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 12466 word types\n",
      "INFO : Tagset: f['W', 'J', 'N', 'C', 'V', 'I', 'D', ',', 'M', 'P', '.', 'E', 'R', '`', \"'\", 'T', '$', ':', '-', '#', 'S', 'F', 'U', 'L', '_EOS_TAG_', '_BOS_TAG_']\n"
     ]
    }
   ],
   "source": [
    "known_vocab = TaggedCorpus(Path(\"ensup\")).vocab    # words seen with supervised tags; used in evaluation\n",
    "log.info(f\"Tagset: f{list(entrain.tagset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an HMM.  Let's do some pre-training to approximately maximize the\n",
    "regularized log-likelihood on supervised training data.  In other words, the\n",
    "probabilities at the M step will just be supervised count ratios.\n",
    "\n",
    "On each epoch, you will see two progress bars: first it collects counts from\n",
    "all the sentences (E step), and then after the M step, it evaluates the loss\n",
    "function, which is the (unregularized) cross-entropy on the training set.\n",
    "\n",
    "The parameters don't actually matter during the E step because there are no\n",
    "hidden tags to impute.  The first M step will jump right to the optimal\n",
    "solution.  The code will try a second epoch with the revised parameters, but\n",
    "the result will be identical, so it will detect convergence and stop.\n",
    "\n",
    "We arbitrarily choose λ=1 for our add-λ smoothing at the M step, but it would\n",
    "be better to search for the best value of this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Hidden Markov Model (HMM)\n",
      "100%|██████████| 996/996 [00:01<00:00, 810.38it/s]\n",
      "INFO : Cross-entropy: 12.6501 nats (= perplexity 311807.579)\n",
      "100%|██████████| 4051/4051 [00:10<00:00, 374.08it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 900.42it/s]\n",
      "INFO : Cross-entropy: 7.5993 nats (= perplexity 1996.789)\n",
      "100%|██████████| 4051/4051 [00:11<00:00, 357.58it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 908.93it/s]\n",
      "INFO : Cross-entropy: 7.5993 nats (= perplexity 1996.791)\n",
      "INFO : Saved model to en_hmm.pkl\n"
     ]
    }
   ],
   "source": [
    "log.info(\"*** Hidden Markov Model (HMM)\")\n",
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab)  # randomly initialized parameters  \n",
    "loss_sup = lambda model: model_cross_entropy(model, eval_corpus=endev)\n",
    "hmm.train(corpus=ensup, loss=loss_sup, λ=1.0,\n",
    "          save_path=\"en_hmm.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log.info(\"*** Hidden Markov Model (HMM)\")\n",
    "# hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab, unigram=True)  # randomly initialized parameters  \n",
    "# loss_sup = lambda model: model_cross_entropy(model, eval_corpus=endev)\n",
    "# hmm.train(corpus=ensup, loss=loss_sup, λ=1.0,\n",
    "#           save_path=\"ensup_hmm_unigram.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_your_data(model, dev, N):\n",
    "    for m, sentence in enumerate(dev):\n",
    "        if m >= N: break\n",
    "        viterbi = model.viterbi_tagging(sentence.desupervise(), endev)\n",
    "        counts = eval_tagging(predicted=viterbi, gold=sentence, \n",
    "                              known_vocab=known_vocab)\n",
    "        num = counts['NUM', 'ALL']\n",
    "        denom = counts['DENOM', 'ALL']\n",
    "        \n",
    "        log.info(f\"Gold:    {sentence}\")\n",
    "        log.info(f\"Viterbi: {viterbi}\")\n",
    "        log.info(f\"Loss:    {denom - num}/{denom}\")\n",
    "        xent = -model.logprob(sentence, endev) / len(sentence)  # measured in nats\n",
    "        log.info(f\"Cross-entropy: {xent/math.log(2)} nats (= perplexity {math.exp(xent)})\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO : Viterbi: ``/` We/P 're/V strongly/D _OOV_/N that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/I this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/, added/V ,/, ``/` and/C that/I means/V virtually/R everyone/, who/W works/V here/R ./.\n",
      "INFO : Loss:    6/34\n",
      "INFO : Cross-entropy: 11.14278507232666 nats (= perplexity 2261.063086657611)\n",
      "---\n",
      "INFO : Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Viterbi: I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/, ``/` _OOV_/P 's/V _OOV_/D _OOV_/N ./. ''/'\n",
      "INFO : Loss:    4/21\n",
      "INFO : Cross-entropy: 11.099005699157715 nats (= perplexity 2193.4796809108902)\n",
      "---\n",
      "INFO : Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Viterbi: It/P is/V the/D _OOV_/N guerrillas/, who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Loss:    2/18\n",
      "INFO : Cross-entropy: 9.995651245117188 nats (= perplexity 1020.9182403729675)\n",
      "---\n",
      "INFO : Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Viterbi: This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Loss:    0/13\n",
      "INFO : Cross-entropy: 9.696027755737305 nats (= perplexity 829.4594086691823)\n",
      "---\n",
      "INFO : Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Viterbi: _OOV_/D _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/I '/P history/N of/I _OOV_/N out/I _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Loss:    4/25\n",
      "INFO : Cross-entropy: 11.261168479919434 nats (= perplexity 2454.422789674587)\n",
      "---\n",
      "INFO : Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Viterbi: Mary/N _OOV_/I Palo/D Alto/N ,/, Calif/N ./.\n",
      "INFO : Loss:    2/7\n",
      "INFO : Cross-entropy: 10.417363166809082 nats (= perplexity 1367.5362846443327)\n",
      "---\n",
      "INFO : Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Viterbi: I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/I well/R as/I other/J U.S./N news/N publications/N of/I like/I mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/M is/V :/: _OOV_/D computer/N equipment/N to/T replace/V that/I damaged/N at/I El/D _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/D _OOV_/J journalists/N ./.\n",
      "INFO : Loss:    8/53\n",
      "INFO : Cross-entropy: 11.92169189453125 nats (= perplexity 3879.598167522698)\n",
      "---\n",
      "INFO : Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Viterbi: Perhaps/I an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/N ''/' El/V _OOV_/T journalists/$ and/C staff/N by/I paying/V for/I added/D security/N in/I exchange/N for/I exclusive/D stories/N ./.\n",
      "INFO : Loss:    7/27\n",
      "INFO : Cross-entropy: 11.795638084411621 nats (= perplexity 3555.0099057538514)\n",
      "---\n",
      "INFO : Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Viterbi: _OOV_/D El/N _OOV_/I 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Loss:    2/9\n",
      "INFO : Cross-entropy: 10.825218200683594 nats (= perplexity 1814.3262527244322)\n",
      "---\n",
      "INFO : Gold:    Douglas/N B./N Evans/N\n",
      "INFO : Viterbi: Douglas/D B./N Evans/.\n",
      "INFO : Loss:    2/3\n",
      "INFO : Cross-entropy: 11.463375091552734 nats (= perplexity 2823.7075273822666)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "look_at_your_data(hmm, endev, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's throw in the unsupervised training data as well, and continue\n",
    "training as before, in order to increase the regularized log-likelihood on\n",
    "this larger, semi-supervised training set.  It's now the *incomplete-data*\n",
    "log-likelihood.\n",
    "\n",
    "This time, we'll use a different evaluation loss function: we'll stop when the\n",
    "*tagging error rate* on a held-out dev set stops getting better.  Also, the\n",
    "implementation of this loss function (`viterbi_error_rate`) includes a helpful\n",
    "side effect: it logs the *cross-entropy* on the held-out dataset as well, just\n",
    "for your information.\n",
    "\n",
    "We hope that held-out tagging accuracy will go up for a little bit before it\n",
    "goes down again (see Merialdo 1994). (Log-likelihood on training data will\n",
    "continue to improve, and that improvement may generalize to held-out\n",
    "cross-entropy.  But getting accuracy to increase is harder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loaded model from en_hmm.pkl\n",
      "100%|██████████| 996/996 [00:01<00:00, 771.91it/s]\n",
      "INFO : Cross-entropy: 7.5993 nats (= perplexity 1996.791)\n",
      "100%|██████████| 996/996 [00:01<00:00, 758.34it/s]\n",
      "INFO : Tagging accuracy: all: 88.663%, known: 93.059%, seen: 44.108%, novel: 42.734%\n",
      "100%|██████████| 8064/8064 [00:23<00:00, 346.55it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 810.04it/s]\n",
      "INFO : Cross-entropy: 7.3485 nats (= perplexity 1553.842)\n",
      "100%|██████████| 996/996 [00:01<00:00, 811.18it/s]\n",
      "INFO : Tagging accuracy: all: 87.031%, known: 91.397%, seen: 45.791%, novel: 40.225%\n",
      "INFO : Saved model to en_hmm_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel.load(\"en_hmm.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n",
    "loss_dev = lambda model: viterbi_error_rate(model, eval_corpus=endev, \n",
    "                                            known_vocab=known_vocab)\n",
    "hmm.train(corpus=entrain, loss=loss_dev, λ=1.0,\n",
    "          save_path=\"en_hmm_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm = HiddenMarkovModel.load(\"ensup_hmm_unigram.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n",
    "# loss_dev = lambda model: viterbi_error_rate(model, eval_corpus=endev, \n",
    "#                                             known_vocab=known_vocab)\n",
    "# hmm.train(corpus=entrain, loss=loss_dev, λ=1.0,\n",
    "#           save_path=\"entrain_hmm_unigram.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also retry the above workflow where you start with a worse supervised\n",
    "model (like Merialdo).  Does EM help more in that case?  It's easiest to rerun\n",
    "exactly the code above, but first make the `ensup` file smaller by copying\n",
    "`ensup-tiny` over it.  `ensup-tiny` is only 25 sentences (that happen to cover\n",
    "all tags in `endev`).  Back up your old `ensup` and your old `*.pkl` models\n",
    "before you do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed look at the first 10 sentences in the held-out corpus,\n",
    "including Viterbi tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO : Viterbi: ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/T added/V ,/, ``/` and/C that/I means/V virtually/R everyone/, who/W works/V here/R ./.\n",
      "INFO : Loss:    3/34\n",
      "INFO : Cross-entropy: 10.617783546447754 nats (= perplexity 1571.344421758747)\n",
      "---\n",
      "INFO : Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Viterbi: I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/, ``/` _OOV_/P 's/V _OOV_/D _OOV_/N ./. ''/'\n",
      "INFO : Loss:    4/21\n",
      "INFO : Cross-entropy: 10.876279830932617 nats (= perplexity 1879.6910462736894)\n",
      "---\n",
      "INFO : Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Viterbi: It/P is/V the/D _OOV_/N guerrillas/, who/W are/V aligned/R with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Loss:    3/18\n",
      "INFO : Cross-entropy: 9.650655746459961 nats (= perplexity 803.7794019158001)\n",
      "---\n",
      "INFO : Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Viterbi: This/D information/N was/V _OOV_/R from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Loss:    1/13\n",
      "INFO : Cross-entropy: 9.343283653259277 nats (= perplexity 649.5440632813669)\n",
      "---\n",
      "INFO : Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Viterbi: _OOV_/D _OOV_/J government/N _OOV_/N of/I the/D ``/N _OOV_/, ''/' was/V due/J to/T the/D drug/N _OOV_/I '/P history/N of/I _OOV_/N out/I _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Loss:    6/25\n",
      "INFO : Cross-entropy: 10.975031852722168 nats (= perplexity 2012.8616183560937)\n",
      "---\n",
      "INFO : Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Viterbi: Mary/N _OOV_/I Palo/D Alto/N ,/, Calif/N ./.\n",
      "INFO : Loss:    2/7\n",
      "INFO : Cross-entropy: 10.526578903198242 nats (= perplexity 1475.081814570695)\n",
      "---\n",
      "INFO : Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Viterbi: I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/I well/R as/I other/J U.S./N news/N publications/N of/I like/I mind/N -RRB-/N should/M put/V its/P money/N where/W its/P mouth/M is/V :/I _OOV_/D computer/N equipment/N to/T replace/V that/I damaged/N at/I El/D _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/D _OOV_/J journalists/N ./.\n",
      "INFO : Loss:    10/53\n",
      "INFO : Cross-entropy: 11.64986515045166 nats (= perplexity 3213.356584783788)\n",
      "---\n",
      "INFO : Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Viterbi: Perhaps/I an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/F ''/' El/V _OOV_/T journalists/$ and/C staff/N by/I paying/V for/I added/J security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Loss:    6/27\n",
      "INFO : Cross-entropy: 11.456049919128418 nats (= perplexity 2809.406877423506)\n",
      "---\n",
      "INFO : Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Viterbi: _OOV_/D El/N _OOV_/I 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Loss:    2/9\n",
      "INFO : Cross-entropy: 10.914258003234863 nats (= perplexity 1929.830500030177)\n",
      "---\n",
      "INFO : Gold:    Douglas/N B./N Evans/N\n",
      "INFO : Viterbi: Douglas/D B./N Evans/.\n",
      "INFO : Loss:    2/3\n",
      "INFO : Cross-entropy: 11.670408248901367 nats (= perplexity 3259.4398878531224)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "look_at_your_data(hmm, endev, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try supervised training of a CRF (this doesn't use the unsupervised\n",
    "part of the data, so it is comparable to the supervised pre-training we did\n",
    "for the HMM).  We will use SGD to approximately maximize the regularized\n",
    "log-likelihood. \n",
    "\n",
    "As with the semi-supervised HMM training, we'll periodically evaluate the\n",
    "tagging accuracy (and also print the cross-entropy) on a held-out dev set.\n",
    "We use the default `eval_interval` and `tolerance`.  If you want to stop\n",
    "sooner, then you could increase the `tolerance` so the training method decides\n",
    "sooner that it has converged.\n",
    "\n",
    "We arbitrarily choose reg = 1.0 for L2 regularization, learning rate = 0.05,\n",
    "and a minibatch size of 10, but it would be better to search for the best\n",
    "value of these hyperparameters.\n",
    "\n",
    "Note that the logger reports the CRF's *conditional* cross-entropy, log p(tags\n",
    "| words) / n.  This is much lower than the HMM's *joint* cross-entropy log\n",
    "p(tags, words) / n, but that doesn't mean the CRF is worse at tagging.  The\n",
    "CRF is just predicting less information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Conditional Random Field (CRF)\n",
      "\n",
      "100%|██████████| 996/996 [00:01<00:00, 540.98it/s]\n",
      "INFO : Cross-entropy: 3.0507 nats (= perplexity 21.129)\n",
      "100%|██████████| 996/996 [00:01<00:00, 920.32it/s]\n",
      "INFO : Tagging accuracy: all: 5.846%, known: 5.980%, seen: 3.030%, novel: 5.020%\n",
      "100%|██████████| 500/500 [00:03<00:00, 141.68it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 543.35it/s]\n",
      "INFO : Cross-entropy: 0.9127 nats (= perplexity 2.491)\n",
      "100%|██████████| 996/996 [00:01<00:00, 941.34it/s]\n",
      "INFO : Tagging accuracy: all: 72.441%, known: 73.623%, seen: 58.081%, novel: 61.030%\n",
      "100%|██████████| 500/500 [00:02<00:00, 183.04it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 586.20it/s]\n",
      "INFO : Cross-entropy: 0.7537 nats (= perplexity 2.125)\n",
      "100%|██████████| 996/996 [00:00<00:00, 1011.53it/s]\n",
      "INFO : Tagging accuracy: all: 75.176%, known: 76.924%, seen: 56.229%, novel: 57.398%\n",
      "100%|██████████| 500/500 [00:02<00:00, 184.02it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 555.02it/s]\n",
      "INFO : Cross-entropy: 0.6610 nats (= perplexity 1.937)\n",
      "100%|██████████| 996/996 [00:01<00:00, 958.24it/s]\n",
      "INFO : Tagging accuracy: all: 78.688%, known: 80.239%, seen: 61.785%, novel: 62.946%\n",
      "100%|██████████| 500/500 [00:02<00:00, 191.96it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 589.54it/s]\n",
      "INFO : Cross-entropy: 0.6079 nats (= perplexity 1.837)\n",
      "100%|██████████| 996/996 [00:01<00:00, 958.07it/s]\n",
      "INFO : Tagging accuracy: all: 80.509%, known: 82.102%, seen: 62.121%, novel: 64.729%\n",
      "100%|██████████| 500/500 [00:02<00:00, 192.43it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 575.36it/s]\n",
      "INFO : Cross-entropy: 0.5733 nats (= perplexity 1.774)\n",
      "100%|██████████| 996/996 [00:01<00:00, 959.61it/s]\n",
      "INFO : Tagging accuracy: all: 80.287%, known: 81.741%, seen: 64.310%, novel: 65.588%\n",
      "100%|██████████| 500/500 [00:02<00:00, 183.92it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 575.95it/s]\n",
      "INFO : Cross-entropy: 0.5418 nats (= perplexity 1.719)\n",
      "100%|██████████| 996/996 [00:01<00:00, 972.44it/s] \n",
      "INFO : Tagging accuracy: all: 81.385%, known: 82.968%, seen: 64.141%, novel: 65.324%\n",
      "100%|██████████| 500/500 [00:02<00:00, 187.21it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 565.35it/s]\n",
      "INFO : Cross-entropy: 0.5163 nats (= perplexity 1.676)\n",
      "100%|██████████| 996/996 [00:01<00:00, 949.65it/s]\n",
      "INFO : Tagging accuracy: all: 83.398%, known: 85.253%, seen: 62.795%, novel: 64.729%\n",
      "100%|██████████| 500/500 [00:02<00:00, 188.37it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 588.84it/s]\n",
      "INFO : Cross-entropy: 0.4947 nats (= perplexity 1.640)\n",
      "100%|██████████| 996/996 [00:00<00:00, 1002.33it/s]\n",
      "INFO : Tagging accuracy: all: 84.133%, known: 86.013%, seen: 63.131%, novel: 65.258%\n",
      "100%|██████████| 500/500 [00:02<00:00, 191.56it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 575.43it/s]\n",
      "INFO : Cross-entropy: 0.4790 nats (= perplexity 1.614)\n",
      "100%|██████████| 996/996 [00:00<00:00, 1000.60it/s]\n",
      "INFO : Tagging accuracy: all: 84.584%, known: 86.512%, seen: 62.963%, novel: 65.258%\n",
      "100%|██████████| 500/500 [00:02<00:00, 193.64it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 570.09it/s]\n",
      "INFO : Cross-entropy: 0.4657 nats (= perplexity 1.593)\n",
      "100%|██████████| 996/996 [00:01<00:00, 995.07it/s] \n",
      "INFO : Tagging accuracy: all: 84.805%, known: 86.846%, seen: 62.458%, novel: 64.135%\n",
      "100%|██████████| 500/500 [00:02<00:00, 194.71it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 589.34it/s]\n",
      "INFO : Cross-entropy: 0.4518 nats (= perplexity 1.571)\n",
      "100%|██████████| 996/996 [00:00<00:00, 1003.78it/s]\n",
      "INFO : Tagging accuracy: all: 85.077%, known: 87.066%, seen: 62.795%, novel: 65.125%\n",
      "100%|██████████| 500/500 [00:02<00:00, 182.61it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 544.42it/s]\n",
      "INFO : Cross-entropy: 0.4426 nats (= perplexity 1.557)\n",
      "100%|██████████| 996/996 [00:00<00:00, 998.91it/s] \n",
      "INFO : Tagging accuracy: all: 85.336%, known: 87.281%, seen: 63.636%, novel: 65.786%\n",
      "100%|██████████| 500/500 [00:02<00:00, 188.90it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 578.73it/s]\n",
      "INFO : Cross-entropy: 0.4311 nats (= perplexity 1.539)\n",
      "100%|██████████| 996/996 [00:00<00:00, 1006.90it/s]\n",
      "INFO : Tagging accuracy: all: 85.741%, known: 87.693%, seen: 63.636%, novel: 66.248%\n",
      "100%|██████████| 500/500 [00:02<00:00, 190.33it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 589.29it/s]\n",
      "INFO : Cross-entropy: 0.4253 nats (= perplexity 1.530)\n",
      "100%|██████████| 996/996 [00:01<00:00, 960.46it/s] \n",
      "INFO : Tagging accuracy: all: 85.761%, known: 87.684%, seen: 64.141%, novel: 66.513%\n",
      "100%|██████████| 500/500 [00:02<00:00, 192.55it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 584.62it/s]\n",
      "INFO : Cross-entropy: 0.4176 nats (= perplexity 1.518)\n",
      "100%|██████████| 996/996 [00:00<00:00, 1004.36it/s]\n",
      "INFO : Tagging accuracy: all: 86.492%, known: 88.435%, seen: 64.646%, novel: 67.041%\n",
      "100%|██████████| 500/500 [00:02<00:00, 189.11it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 592.04it/s]\n",
      "INFO : Cross-entropy: 0.4087 nats (= perplexity 1.505)\n",
      "100%|██████████| 996/996 [00:01<00:00, 967.74it/s]\n",
      "INFO : Tagging accuracy: all: 86.066%, known: 88.251%, seen: 63.131%, novel: 63.540%\n",
      "INFO : Saved model to en_crf.pkl\n"
     ]
    }
   ],
   "source": [
    "log.info(\"*** Conditional Random Field (CRF)\\n\")\n",
    "crf = ConditionalRandomField(entrain.tagset, entrain.vocab)  # randomly initialized parameters  \n",
    "crf.train(corpus=ensup, loss=loss_dev, reg=1.0, lr=0.05, minibatch_size=10,\n",
    "          save_path=\"en_crf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how the CRF does on individual sentences. \n",
    "(Do you see any error patterns here that would inspire additional CRF features?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Conditional Random Field (CRF)\n",
      "\n",
      "INFO : Loaded model from en_crf.pkl\n",
      "100%|██████████| 996/996 [00:01<00:00, 498.78it/s]\n",
      "INFO : Cross-entropy: 0.4087 nats (= perplexity 1.505)\n",
      "100%|██████████| 996/996 [00:01<00:00, 870.96it/s]\n",
      "INFO : Tagging accuracy: all: 86.066%, known: 88.251%, seen: 63.131%, novel: 63.540%\n",
      "100%|██████████| 500/500 [00:03<00:00, 159.00it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 481.61it/s]\n",
      "INFO : Cross-entropy: 0.4074 nats (= perplexity 1.503)\n",
      "100%|██████████| 996/996 [00:01<00:00, 861.12it/s]\n",
      "INFO : Tagging accuracy: all: 86.121%, known: 88.036%, seen: 64.983%, novel: 66.777%\n",
      "100%|██████████| 500/500 [00:02<00:00, 188.87it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 509.26it/s]\n",
      "INFO : Cross-entropy: 0.4023 nats (= perplexity 1.495)\n",
      "100%|██████████| 996/996 [00:01<00:00, 862.61it/s]\n",
      "INFO : Tagging accuracy: all: 86.772%, known: 88.888%, seen: 63.300%, novel: 65.456%\n",
      "100%|██████████| 500/500 [00:02<00:00, 181.86it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 487.52it/s]\n",
      "INFO : Cross-entropy: 0.3974 nats (= perplexity 1.488)\n",
      "100%|██████████| 996/996 [00:01<00:00, 859.82it/s]\n",
      "INFO : Tagging accuracy: all: 86.918%, known: 89.030%, seen: 62.963%, novel: 65.852%\n",
      "100%|██████████| 500/500 [00:02<00:00, 184.63it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 484.84it/s]\n",
      "INFO : Cross-entropy: 0.3981 nats (= perplexity 1.489)\n",
      "100%|██████████| 996/996 [00:01<00:00, 864.59it/s]\n",
      "INFO : Tagging accuracy: all: 87.348%, known: 89.749%, seen: 62.626%, novel: 62.417%\n",
      "100%|██████████| 500/500 [00:02<00:00, 177.47it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 491.82it/s]\n",
      "INFO : Cross-entropy: 0.3923 nats (= perplexity 1.480)\n",
      "100%|██████████| 996/996 [00:01<00:00, 861.66it/s]\n",
      "INFO : Tagging accuracy: all: 86.860%, known: 89.039%, seen: 62.795%, novel: 64.861%\n",
      "100%|██████████| 500/500 [00:02<00:00, 182.14it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 504.77it/s]\n",
      "INFO : Cross-entropy: 0.3914 nats (= perplexity 1.479)\n",
      "100%|██████████| 996/996 [00:01<00:00, 872.65it/s]\n",
      "INFO : Tagging accuracy: all: 86.396%, known: 88.416%, seen: 63.300%, novel: 66.314%\n",
      "100%|██████████| 500/500 [00:02<00:00, 186.21it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 508.56it/s]\n",
      "INFO : Cross-entropy: 0.3860 nats (= perplexity 1.471)\n",
      "100%|██████████| 996/996 [00:01<00:00, 865.64it/s]\n",
      "INFO : Tagging accuracy: all: 86.922%, known: 89.016%, seen: 64.141%, novel: 65.654%\n",
      "100%|██████████| 500/500 [00:02<00:00, 168.46it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 468.51it/s]\n",
      "INFO : Cross-entropy: 0.3847 nats (= perplexity 1.469)\n",
      "100%|██████████| 996/996 [00:01<00:00, 652.78it/s]\n",
      "INFO : Tagging accuracy: all: 87.770%, known: 90.028%, seen: 62.963%, novel: 64.927%\n",
      "100%|██████████| 500/500 [00:02<00:00, 170.34it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 468.34it/s]\n",
      "INFO : Cross-entropy: 0.3819 nats (= perplexity 1.465)\n",
      "100%|██████████| 996/996 [00:01<00:00, 739.50it/s]\n",
      "INFO : Tagging accuracy: all: 86.622%, known: 88.705%, seen: 63.973%, novel: 65.456%\n",
      "100%|██████████| 500/500 [00:02<00:00, 186.43it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 503.77it/s]\n",
      "INFO : Cross-entropy: 0.3799 nats (= perplexity 1.462)\n",
      "100%|██████████| 996/996 [00:01<00:00, 881.99it/s]\n",
      "INFO : Tagging accuracy: all: 87.503%, known: 89.726%, seen: 63.131%, novel: 64.993%\n",
      "100%|██████████| 500/500 [00:02<00:00, 181.12it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 509.48it/s]\n",
      "INFO : Cross-entropy: 0.3805 nats (= perplexity 1.463)\n",
      "100%|██████████| 996/996 [00:01<00:00, 885.93it/s]\n",
      "INFO : Tagging accuracy: all: 86.605%, known: 88.714%, seen: 63.131%, novel: 65.390%\n",
      "100%|██████████| 500/500 [00:02<00:00, 190.72it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 512.50it/s]\n",
      "INFO : Cross-entropy: 0.3761 nats (= perplexity 1.457)\n",
      "100%|██████████| 996/996 [00:01<00:00, 865.31it/s]\n",
      "INFO : Tagging accuracy: all: 86.713%, known: 88.833%, seen: 62.458%, novel: 65.654%\n",
      "100%|██████████| 500/500 [00:02<00:00, 184.04it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 503.77it/s]\n",
      "INFO : Cross-entropy: 0.3755 nats (= perplexity 1.456)\n",
      "100%|██████████| 996/996 [00:01<00:00, 876.78it/s]\n",
      "INFO : Tagging accuracy: all: 86.885%, known: 89.007%, seen: 63.300%, novel: 65.522%\n",
      "100%|██████████| 500/500 [00:02<00:00, 188.28it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 507.13it/s]\n",
      "INFO : Cross-entropy: 0.3712 nats (= perplexity 1.450)\n",
      "100%|██████████| 996/996 [00:01<00:00, 883.24it/s]\n",
      "INFO : Tagging accuracy: all: 87.941%, known: 90.106%, seen: 63.131%, novel: 66.446%\n",
      "100%|██████████| 500/500 [00:02<00:00, 183.28it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 498.22it/s]\n",
      "INFO : Cross-entropy: 0.3718 nats (= perplexity 1.450)\n",
      "100%|██████████| 996/996 [00:01<00:00, 856.18it/s]\n",
      "INFO : Tagging accuracy: all: 87.791%, known: 89.900%, seen: 63.300%, novel: 66.975%\n",
      "100%|██████████| 500/500 [00:02<00:00, 179.08it/s]\n",
      "100%|██████████| 996/996 [00:01<00:00, 504.35it/s]\n",
      "INFO : Cross-entropy: 0.3685 nats (= perplexity 1.445)\n",
      "100%|██████████| 996/996 [00:01<00:00, 873.56it/s]\n",
      "INFO : Tagging accuracy: all: 87.594%, known: 89.854%, seen: 62.458%, novel: 64.861%\n",
      "100%|██████████| 500/500 [00:02<00:00, 175.93it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 496.27it/s]\n",
      "INFO : Cross-entropy: 0.3655 nats (= perplexity 1.441)\n",
      "100%|██████████| 996/996 [00:01<00:00, 868.44it/s]\n",
      "INFO : Tagging accuracy: all: 87.903%, known: 90.152%, seen: 62.963%, novel: 65.258%\n",
      "100%|██████████| 500/500 [00:02<00:00, 174.20it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 477.43it/s]\n",
      "INFO : Cross-entropy: 0.3641 nats (= perplexity 1.439)\n",
      "100%|██████████| 996/996 [00:01<00:00, 875.62it/s]\n",
      "INFO : Tagging accuracy: all: 88.004%, known: 90.261%, seen: 62.795%, novel: 65.324%\n",
      "100%|██████████| 500/500 [00:02<00:00, 188.38it/s]\n",
      "100%|██████████| 996/996 [00:02<00:00, 477.96it/s]\n",
      "INFO : Cross-entropy: 0.3652 nats (= perplexity 1.441)\n",
      "100%|██████████| 996/996 [00:01<00:00, 790.56it/s]\n",
      "INFO : Tagging accuracy: all: 87.878%, known: 89.968%, seen: 63.636%, novel: 67.239%\n",
      "INFO : Saved model to en_crf_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "log.info(\"*** Conditional Random Field (CRF)\\n\")\n",
    "crf = ConditionalRandomField.load(\"en_crf.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n",
    "# crf = ConditionalRandomField(entrain.tagset, entrain.vocab)  # randomly initialized parameters  \n",
    "crf.train(corpus=entrain, loss=loss_dev, reg=1.0, lr=0.05, minibatch_size=10,\n",
    "          save_path=\"en_crf_raw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO : Viterbi: ``/` We/P 're/V strongly/J _OOV_/N that/I anyone/N who/W has/V eaten/N in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/I means/J virtually/N everyone/N who/W works/V here/R ./.\n",
      "INFO : Loss:    6/34\n",
      "INFO : Cross-entropy: 0.6837345361709595 nats (= perplexity 1.6062923902886483)\n",
      "---\n",
      "INFO : Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Viterbi: I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/J Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/J _OOV_/N ./. ''/'\n",
      "INFO : Loss:    2/21\n",
      "INFO : Cross-entropy: 0.4155416786670685 nats (= perplexity 1.333799382511529)\n",
      "---\n",
      "INFO : Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Viterbi: It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/N with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Loss:    1/18\n",
      "INFO : Cross-entropy: 0.2750522494316101 nats (= perplexity 1.2100379223475384)\n",
      "---\n",
      "INFO : Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Viterbi: This/D information/N was/V _OOV_/N from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Loss:    1/13\n",
      "INFO : Cross-entropy: 0.3502766489982605 nats (= perplexity 1.2748050652302296)\n",
      "---\n",
      "INFO : Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Viterbi: _OOV_/N _OOV_/N government/N _OOV_/N of/I the/D ``/` _OOV_/N ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/N out/I _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Loss:    5/25\n",
      "INFO : Cross-entropy: 0.8285244107246399 nats (= perplexity 1.7758681136921284)\n",
      "---\n",
      "INFO : Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Viterbi: Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Loss:    0/7\n",
      "INFO : Cross-entropy: 0.4782157242298126 nats (= perplexity 1.3930197698317561)\n",
      "---\n",
      "INFO : Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Viterbi: I/P suggest/N that/I The/D Wall/N Street/N Journal/N -LRB-/- as/I well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/N computer/N equipment/N to/T replace/V that/I damaged/N at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/N stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Loss:    5/53\n",
      "INFO : Cross-entropy: 0.5918160080909729 nats (= perplexity 1.507142714506011)\n",
      "---\n",
      "INFO : Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Viterbi: Perhaps/I an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/N ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/N for/I added/J security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Loss:    4/27\n",
      "INFO : Cross-entropy: 0.750611424446106 nats (= perplexity 1.6825057092976026)\n",
      "---\n",
      "INFO : Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Viterbi: _OOV_/N El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Loss:    1/9\n",
      "INFO : Cross-entropy: 0.8285890817642212 nats (= perplexity 1.7759477146671196)\n",
      "---\n",
      "INFO : Gold:    Douglas/N B./N Evans/N\n",
      "INFO : Viterbi: Douglas/N B./N Evans/.\n",
      "INFO : Loss:    1/3\n",
      "INFO : Cross-entropy: 0.7248554229736328 nats (= perplexity 1.6527350338337803)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "look_at_your_data(crf, endev, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
